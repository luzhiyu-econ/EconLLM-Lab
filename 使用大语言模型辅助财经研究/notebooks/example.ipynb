{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础调用示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI调用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 填入你的实际 API Key\n",
    "client = OpenAI(api_key=\"<OpenAI API Key>\", base_url=\"https://api.openai.com/v1\")\n",
    "\n",
    "prompt = \"You are a helpful assistant\"\n",
    "input_text = \"What is the capital of France?\"\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "    {\"role\": \"system\",\"content\": prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": input_text\n",
    "    }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=2048,  # 修改为 max_tokens\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    ")\n",
    "\n",
    "# 打印响应内容\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSeek调用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这段评论中的“顶王粉丝光荣站岗”属于**消极**情绪。\n",
      "\n",
      "**分析依据**：\n",
      "1. **\"站岗\"**：在股市用语中常指投资者在高位买入股票后股价下跌，被迫长期持有（类似哨兵站岗无法离开），带有套牢的负面含义。\n",
      "2. **\"光荣\"**：此处为反语，表面褒义实则表达自嘲，暗示被迫坚持持有亏损头寸的无奈。\n",
      "3. **整体语境**：结合股民社区常见表达习惯，该评论通过调侃方式反映对持仓亏损的不满，情绪偏向消极。\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 填入你的实际 API Key\n",
    "client = OpenAI(api_key=\"<DeepSeek API Key>\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "prompt = \"你是一名文本情绪分析专家\"\n",
    "input_text = \"请你判断这段股民评论属于：积极、消极还是相对中性？#留言：顶王粉丝光荣站岗\"\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=[\n",
    "    {\"role\": \"system\",\"content\": prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": input_text\n",
    "    }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=2048,  # 修改为 max_tokens\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    ")\n",
    "\n",
    "# 打印响应内容\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多线程调用示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本代码默认要求输出格式为json格式的词典：json\\n{\\n  \\\"economic_growth_goal\\\": \\\"7.5%\\\"\\n}\n",
    "### 可根据需求修改JSON解析模块以适应不同情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# -------------------------------\n",
    "# JSON解析模块（独立模块）\n",
    "# -------------------------------\n",
    "def default_json_parser(content, idx=None):\n",
    "    \"\"\"\n",
    "    默认的 JSON 解析器：\n",
    "    清理输入内容后尝试解析 JSON，\n",
    "    若成功则返回完整的字典，若失败返回空字典。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 去除代码块标记，清理内容\n",
    "        cleaned_content = content.replace('```json\\n', '').replace('```', '').strip()\n",
    "        parsed_result = json.loads(cleaned_content)\n",
    "        return parsed_result\n",
    "    except json.JSONDecodeError:\n",
    "        if idx is not None:\n",
    "            logging.warning(f\"警告: 第 {idx} 行解析 JSON 失败\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        if idx is not None:\n",
    "            logging.error(f\"错误: 第 {idx} 行解析失败 - {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "# -------------------------------\n",
    "# 限流处理器（控制请求频率）\n",
    "# -------------------------------\n",
    "class RateLimitedProcessor:\n",
    "    def __init__(self):\n",
    "        self.request_timestamps = []\n",
    "        self.MAX_RPM = 500\n",
    "        self.window_size = 60  # 60秒窗口\n",
    "\n",
    "    def _clean_old_records(self, current_time):\n",
    "        cutoff_time = current_time - timedelta(seconds=self.window_size)\n",
    "        self.request_timestamps = [ts for ts in self.request_timestamps if ts > cutoff_time]\n",
    "\n",
    "    def can_make_request(self):\n",
    "        \"\"\"检查是否可以发起新请求\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        self._clean_old_records(current_time)\n",
    "        if len(self.request_timestamps) >= self.MAX_RPM:\n",
    "            return False\n",
    "        self.request_timestamps.append(current_time)\n",
    "        return True\n",
    "\n",
    "# -------------------------------\n",
    "# OpenAI文本处理器\n",
    "# -------------------------------\n",
    "class OpenAITextProcessor:\n",
    "    def __init__(self, api_key=None, model=None, base_url=None, json_parser=None):\n",
    "        self.client = OpenAI(api_key=api_key,base_url=base_url)\n",
    "        self.model = model\n",
    "        self.rate_limiter = RateLimitedProcessor()\n",
    "        self.n_workers = 14  # 优化后的线程数\n",
    "        # 如果未提供自定义解析器，则使用默认解析器\n",
    "        self.json_parser = json_parser if json_parser is not None else default_json_parser\n",
    "\n",
    "    def process_batch(self, df, text_column, prompt, batch_size=20, delay=1, json_parser=None):\n",
    "        \"\"\"\n",
    "        批量处理文本，支持灵活的 JSON 解析。\n",
    "        \n",
    "        参数:\n",
    "            df: 包含文本数据的 DataFrame\n",
    "            text_column: 文本所在的列名\n",
    "            prompt: 系统提示，用于 API 调用\n",
    "            batch_size: 每个批次处理的文本条数\n",
    "            delay: 每次请求后的延迟（秒）\n",
    "            json_parser: 可选的自定义 JSON 解析器，若不传入则使用实例内的解析器\n",
    "        \n",
    "        返回:\n",
    "            新的 DataFrame，包含原始数据及 API 返回结果（通过 JSON 解析获得的各字段）\n",
    "        \"\"\"\n",
    "        parser = json_parser if json_parser is not None else self.json_parser\n",
    "        results = []  # 保存每次请求解析后的结果（字典形式）\n",
    "\n",
    "        def process_chunk(chunk_data):\n",
    "            chunk_results = []\n",
    "            for idx, text in chunk_data:\n",
    "                # 限流检测：等待直到可以发送请求\n",
    "                while not self.rate_limiter.can_make_request():\n",
    "                    time.sleep(0.1)\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": prompt},\n",
    "                            {\"role\": \"user\", \"content\": text}\n",
    "                        ],\n",
    "                        temperature=0,\n",
    "                        max_tokens=40\n",
    "                    )\n",
    "                    # 使用解析器处理响应内容，得到字典格式结果\n",
    "                    parsed_result = parser(response.choices[0].message.content, idx)\n",
    "                    chunk_results.append(parsed_result)\n",
    "                    time.sleep(delay)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"错误: 处理第 {idx} 行时发生异常: {str(e)}\")\n",
    "                    chunk_results.append({})\n",
    "            return chunk_results\n",
    "\n",
    "        # 将数据分成批次，保留行号信息\n",
    "        chunks = [\n",
    "            list(enumerate(df[text_column][i:i+batch_size]))\n",
    "            for i in range(0, len(df), batch_size)\n",
    "        ]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.n_workers) as executor:\n",
    "            futures = list(tqdm(\n",
    "                executor.map(process_chunk, chunks),\n",
    "                total=len(chunks),\n",
    "                desc=\"Processing batches\"\n",
    "            ))\n",
    "            for chunk_results in futures:\n",
    "                results.extend(chunk_results)\n",
    "\n",
    "        # 将解析结果列表转为 DataFrame，并与原 DataFrame 合并\n",
    "        df_result = df.copy().reset_index(drop=True)\n",
    "        results_df = pd.json_normalize(results)\n",
    "        df_result = pd.concat([df_result, results_df], axis=1)\n",
    "\n",
    "        # 统计处理情况\n",
    "        success_count = sum(1 for r in results if r)\n",
    "        total_count = len(results)\n",
    "        success_rate = (success_count / total_count) * 100 if total_count > 0 else 0\n",
    "        logging.info(f\"处理完成: 总数 {total_count}, 成功 {success_count}, 成功率 {success_rate:.2f}%\")\n",
    "        \n",
    "        return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 20/20 [00:24<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "processor = OpenAITextProcessor(api_key=\"<OpenAI API Key>\", base_url=\"https://api.openai.com/v1\",model=\"gpt-4o-mini\")\n",
    "df_result = processor.process_batch(\n",
    "    df=pd.read_csv(\"\"),#读取待处理的csv文件\n",
    "    text_column=\"\",#待处理的列名\n",
    "    prompt=prompt,#提示词\n",
    "    batch_size=5,#批处理大小\n",
    ")\n",
    "df_result.to_csv(\"\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实际处理案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建规范表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame创建成功！\n",
      "\n",
      "数据预览：\n",
      "    城市    年份                                               文本内容\n",
      "0  上海市  2016           \\n\\t上海市市长杨雄\\n\\n\\t各位代表：\\n\\n\\t　　现在，我代表上...\n",
      "1  上海市  2017           \\n\\t上海市市长杨雄\\n\\n\\t各位代表：\\n\\n\\t　　现在，我代表上...\n",
      "2  上海市  2015  \\n\\t——2015年1月25日在上海市第十四届人民代表大会第三次会议上\\n\\n\\t上海市市...\n",
      "3  上海市  2014          \\n\\t　　各位代表：\\n\\n\\t　　现在，我代表上海市人民政府，向大会作政...\n",
      "4  上海市  2010  \\n\\n                            \\n\\n          ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_txt_files(base_path='data/政府工作报告'):\n",
    "    # 创建空列表来存储数据\n",
    "    data = []\n",
    "    \n",
    "    # 遍历城市文件夹\n",
    "    for city in os.listdir(base_path):\n",
    "        city_path = os.path.join(base_path, city)\n",
    "        \n",
    "        # 确保是目录\n",
    "        if os.path.isdir(city_path):\n",
    "            # 遍历年份文件\n",
    "            for file_name in os.listdir(city_path):\n",
    "                if file_name.endswith('.txt'):\n",
    "                    # 获取年份（去掉.txt后缀）\n",
    "                    year = file_name.replace('.txt', '')\n",
    "                    \n",
    "                    # 构建完整的文件路径\n",
    "                    file_path = os.path.join(city_path, file_name)\n",
    "                    \n",
    "                    try:\n",
    "                        # 读取文件内容\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "                            \n",
    "                        # 将数据添加到列表中\n",
    "                        data.append({\n",
    "                            '城市': city,\n",
    "                            '年份': year,\n",
    "                            '文本内容': content\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"读取文件 {file_path} 时出错: {str(e)}\")\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 使用函数\n",
    "try:\n",
    "    df = read_txt_files()\n",
    "    print(\"DataFrame创建成功！\")\n",
    "    print(\"\\n数据预览：\")\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"处理过程中出错: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用大语言模型批量处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# -------------------------------\n",
    "# JSON解析模块（独立模块）\n",
    "# -------------------------------\n",
    "def default_json_parser(content, idx=None):\n",
    "    \"\"\"\n",
    "    默认的 JSON 解析器：\n",
    "    清理输入内容后尝试解析 JSON，\n",
    "    若成功则返回完整的字典，若失败返回空字典。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 去除代码块标记，清理内容\n",
    "        cleaned_content = content.replace('```json\\n', '').replace('```', '').strip()\n",
    "        parsed_result = json.loads(cleaned_content)\n",
    "        return parsed_result\n",
    "    except json.JSONDecodeError:\n",
    "        if idx is not None:\n",
    "            logging.warning(f\"警告: 第 {idx} 行解析 JSON 失败\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        if idx is not None:\n",
    "            logging.error(f\"错误: 第 {idx} 行解析失败 - {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "# -------------------------------\n",
    "# 限流处理器（控制请求频率）\n",
    "# -------------------------------\n",
    "class RateLimitedProcessor:\n",
    "    def __init__(self):\n",
    "        self.request_timestamps = []\n",
    "        self.MAX_RPM = 500\n",
    "        self.window_size = 60  # 60秒窗口\n",
    "\n",
    "    def _clean_old_records(self, current_time):\n",
    "        cutoff_time = current_time - timedelta(seconds=self.window_size)\n",
    "        self.request_timestamps = [ts for ts in self.request_timestamps if ts > cutoff_time]\n",
    "\n",
    "    def can_make_request(self):\n",
    "        \"\"\"检查是否可以发起新请求\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        self._clean_old_records(current_time)\n",
    "        if len(self.request_timestamps) >= self.MAX_RPM:\n",
    "            return False\n",
    "        self.request_timestamps.append(current_time)\n",
    "        return True\n",
    "\n",
    "# -------------------------------\n",
    "# OpenAI文本处理器\n",
    "# -------------------------------\n",
    "class OpenAITextProcessor:\n",
    "    def __init__(self, api_key=None, model=None, base_url=None, json_parser=None):\n",
    "        self.client = OpenAI(api_key=api_key,base_url=base_url)\n",
    "        self.model = model\n",
    "        self.rate_limiter = RateLimitedProcessor()\n",
    "        self.n_workers = 14  # 优化后的线程数\n",
    "        # 如果未提供自定义解析器，则使用默认解析器\n",
    "        self.json_parser = json_parser if json_parser is not None else default_json_parser\n",
    "\n",
    "    def process_batch(self, df, text_column, prompt, batch_size=20, delay=1, json_parser=None):\n",
    "        \"\"\"\n",
    "        批量处理文本，支持灵活的 JSON 解析。\n",
    "        \n",
    "        参数:\n",
    "            df: 包含文本数据的 DataFrame\n",
    "            text_column: 文本所在的列名\n",
    "            prompt: 系统提示，用于 API 调用\n",
    "            batch_size: 每个批次处理的文本条数\n",
    "            delay: 每次请求后的延迟（秒）\n",
    "            json_parser: 可选的自定义 JSON 解析器，若不传入则使用实例内的解析器\n",
    "        \n",
    "        返回:\n",
    "            新的 DataFrame，包含原始数据及 API 返回结果（通过 JSON 解析获得的各字段）\n",
    "        \"\"\"\n",
    "        parser = json_parser if json_parser is not None else self.json_parser\n",
    "        results = []  # 保存每次请求解析后的结果（字典形式）\n",
    "\n",
    "        def process_chunk(chunk_data):\n",
    "            chunk_results = []\n",
    "            for idx, text in chunk_data:\n",
    "                # 限流检测：等待直到可以发送请求\n",
    "                while not self.rate_limiter.can_make_request():\n",
    "                    time.sleep(0.1)\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": prompt},\n",
    "                            {\"role\": \"user\", \"content\": text}\n",
    "                        ],\n",
    "                        temperature=0,\n",
    "                        max_tokens=40\n",
    "                    )\n",
    "                    # 使用解析器处理响应内容，得到字典格式结果\n",
    "                    parsed_result = parser(response.choices[0].message.content, idx)\n",
    "                    chunk_results.append(parsed_result)\n",
    "                    time.sleep(delay)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"错误: 处理第 {idx} 行时发生异常: {str(e)}\")\n",
    "                    chunk_results.append({})\n",
    "            return chunk_results\n",
    "\n",
    "        # 将数据分成批次，保留行号信息\n",
    "        chunks = [\n",
    "            list(enumerate(df[text_column][i:i+batch_size]))\n",
    "            for i in range(0, len(df), batch_size)\n",
    "        ]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.n_workers) as executor:\n",
    "            futures = list(tqdm(\n",
    "                executor.map(process_chunk, chunks),\n",
    "                total=len(chunks),\n",
    "                desc=\"Processing batches\"\n",
    "            ))\n",
    "            for chunk_results in futures:\n",
    "                results.extend(chunk_results)\n",
    "\n",
    "        # 将解析结果列表转为 DataFrame，并与原 DataFrame 合并\n",
    "        df_result = df.copy().reset_index(drop=True)\n",
    "        results_df = pd.json_normalize(results)\n",
    "        df_result = pd.concat([df_result, results_df], axis=1)\n",
    "\n",
    "        # 统计处理情况\n",
    "        success_count = sum(1 for r in results if r)\n",
    "        total_count = len(results)\n",
    "        success_rate = (success_count / total_count) * 100 if total_count > 0 else 0\n",
    "        logging.info(f\"处理完成: 总数 {total_count}, 成功 {success_count}, 成功率 {success_rate:.2f}%\")\n",
    "        \n",
    "        return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= \"从政府采购文本中识别该市当年经济增长目标，并将其以规范的JSON格式输出。确保信息准确无误，并正确识别该政府的经济增长目标。\\n\\n# Steps\\n\\n1. 阅读并理解给定的政府采购文本。\\n2. 查找与该市当年经济增长目标相关的信息。\\n3. 提取相关信息，确保其准确反映文本中的内容。\\n4. 使用规范的JSON格式输出信息。\\n\\n# Output Format\\n\\n输出应为JSON格式，包含以下结构：\\n\\n```json\\n{\\n  \\\"economic_growth_goal\\\": \\\"[经济增长目标百分比]\\\"\\n}\\n```\\n\\n# Examples\\n\\n**Input:** \\n政府采购文本的部分内容为：“今年本市的经济增长目标为7.5%。”\\n\\n**Output:**\\n\\n```json\\n{\\n  \\\"economic_growth_goal\\\": \\\"7.5%\\\"\\n}\\n```\\n#Notes\\n1. 增长目标以数字+%的形式输出\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 4/4 [02:31<00:00, 37.86s/it]\n"
     ]
    }
   ],
   "source": [
    "processor = OpenAITextProcessor(api_key=\"<DeepSeek API Key>\", base_url=\"https://api.deepseek.com\",model=\"deepseek-chat\")\n",
    "df_result = processor.process_batch(\n",
    "    df=df,\n",
    "    text_column=\"文本内容\",\n",
    "    prompt=prompt,\n",
    "    batch_size=5,\n",
    ")\n",
    "df_result.to_csv(\"data/提取结果.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
